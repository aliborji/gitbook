---
description: some general deep learning techniques
---

# Deep Learning

### Optimizer

* Stochastic Gradient Descent \(SGD\)
* RMSProp
* Momentum
* AdaGrad
* Adam [\[1412.6980\]](https://arxiv.org/abs/1412.6980)
* RAdam [\[Github\]](https://github.com/LiyuanLucasLiu/RAdam) [\[1908.03265\]](https://arxiv.org/abs/1908.03265)

### Normalization

* Batch Normalization
* Instance Normalization
* Group Normalization
* Spectral Normalization

### Weight Initialization

* Kaming init

### Activation Function

* Sigmoid
  * $$f(x) = \frac{1}{1+e^{-x}}$$
* ReLU
* Leaky ReLU
* Tanh
* Swish [\[1710.05941\]](https://arxiv.org/abs/1710.05941)
* Mish [\[Github\]](https://github.com/digantamisra98/Mish) [\[1908.08681\]](https://arxiv.org/abs/1908.08681)

### Frameworks

* [Tensorflow](https://www.tensorflow.org/) / [Keras](https://keras.io/)
* [Pytorch](https://pytorch.org/) / [fastai](https://docs.fast.ai/)
* [MXNet](https://mxnet.apache.org/) / [Gluon](https://gluon.mxnet.io/)
* hardly used now \(2019.11\)
  * [Caffe](https://caffe.berkeleyvision.org/)
  * [Caffe2](https://caffe2.ai/) \(now part of Pytorch\)
  * [Torch](http://torch.ch/) \(lua\)
  * [Matlab](https://www.mathworks.com/products/matlab.html) / [Octave](https://www.gnu.org/software/octave/)

