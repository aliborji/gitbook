---
description: some general deep learning techniques
---

# Deep Learning

## Survey Papers / Books

* Deep Learning Book [\[deeplearningbook.org\]](https://www.deeplearningbook.org/)
* Deep learning [\[Nature\]](https://www.nature.com/articles/nature14539)

## Resources

### Courses

* [Coursera Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by Andrew Ng

### Frameworks

* [TensorFlow](https://www.tensorflow.org/) / [Keras](https://keras.io/)
* [PyTorch](https://pytorch.org/) / [fastai](https://docs.fast.ai/)
* [mxnet](https://mxnet.apache.org/) / [GLUON](https://gluon.mxnet.io/)
* hardly used now \(2019.11\)
  * [Caffe](https://caffe.berkeleyvision.org/)
  * [Caffe2](https://caffe2.ai/) \(now part of PyTorch\)
  * [Torch](http://torch.ch/) \(Lua\)
  * [Matlab](https://www.mathworks.com/products/matlab.html) / [Octave](https://www.gnu.org/software/octave/)

## Models

### Optimizer

* Stochastic Gradient Descent \(SGD\)
* RMSProp
* Momentum
* AdaGrad
* Adam [\[1412.6980\]](https://arxiv.org/abs/1412.6980)
* AdamW [\[1711.05101\]](https://arxiv.org/abs/1711.05101)
* AdaBound [\[1902.09843\]](https://arxiv.org/abs/1902.09843)
* AMSGrad [\[1904.09237\]](https://arxiv.org/abs/1904.09237)
* RAdam [\[LiyuanLucasLiu/RAdam\]](https://github.com/LiyuanLucasLiu/RAdam) [\[1908.03265\]](https://arxiv.org/abs/1908.03265)

### Normalization

* Batch Normalization [\[1502.03167\]](https://arxiv.org/abs/1502.03167)
* Weight Normalization [\[1602.07868\]](https://arxiv.org/abs/1602.07868)
* Layer Normalization [\[1607.06450\]](https://arxiv.org/abs/1607.06450)
* Instance Normalization [\[1607.08022\]](https://arxiv.org/abs/1607.08022)
* Spectral Normalization [\[1802.05957\]](https://arxiv.org/abs/1802.05957)
* Group Normalization [\[1803.08494\]](https://arxiv.org/abs/1803.08494)
* Switchable Normalization [\[1806.10779\]](https://arxiv.org/abs/1806.10779)

### Weight Initialization

* Xavier Initialization [\[JMLR'10\]](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)
* He Initialization [\[1502.01852\]](https://arxiv.org/abs/1502.01852)

### Activation Function

* Sigmoid
* Tanh
* ReLU
* Leaky ReLU
* Swish [\[1710.05941\]](https://arxiv.org/abs/1710.05941)
* Mish [\[digantamisra98/Mish\]](https://github.com/digantamisra98/Mish) [\[1908.08681\]](https://arxiv.org/abs/1908.08681)

### Layers

* Softmax
* Soft argmax
* Gumbel Softmax [\[1611.01144\]](https://arxiv.org/abs/1611.01144)

