---
description: some general deep learning techniques
---

# Deep Learning

## Survey Papers

* Deep Learning Book [\[deeplearningbook.org\]](https://www.deeplearningbook.org/)

## Resources

### Frameworks

* [TensorFlow](https://www.tensorflow.org/) / [Keras](https://keras.io/)
* [PyTorch](https://pytorch.org/) / [fastai](https://docs.fast.ai/)
* [mxnet](https://mxnet.apache.org/) / [GLUON](https://gluon.mxnet.io/)
* hardly used now \(2019.11\)
  * [Caffe](https://caffe.berkeleyvision.org/)
  * [Caffe2](https://caffe2.ai/) \(now part of PyTorch\)
  * [Torch](http://torch.ch/) \(Lua\)
  * [Matlab](https://www.mathworks.com/products/matlab.html) / [Octave](https://www.gnu.org/software/octave/)

## Models

### Optimizer

* Stochastic Gradient Descent \(SGD\)
* RMSProp
* Momentum
* AdaGrad
* Adam [\[1412.6980\]](https://arxiv.org/abs/1412.6980)
* RAdam [\[LiyuanLucasLiu/RAdam\]](https://github.com/LiyuanLucasLiu/RAdam) [\[1908.03265\]](https://arxiv.org/abs/1908.03265)

### Normalization

* Batch Normalization
* Instance Normalization
* Group Normalization
* Spectral Normalization

### Weight Initialization

* Kaming init

### Activation Function

* Sigmoid
  * $$f(x) = \frac{1}{1+e^{-x}}$$
* Tanh
* ReLU
* Leaky ReLU
* Swish [\[1710.05941\]](https://arxiv.org/abs/1710.05941)
* Mish [\[digantamisra98/Mish\]](https://github.com/digantamisra98/Mish) [\[1908.08681\]](https://arxiv.org/abs/1908.08681)

